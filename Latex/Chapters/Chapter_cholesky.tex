% Chapter Template

\chapter{Cholesky Factorization} % Main chapter title

\label{ChapterX} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
Cholesky factorization is an important term in the feld of linear algebra. It is the process of decomposing a Hermitian, positive-definite matrix into product of a lower and upper triangular matrix and its conjygate transpose. Which is useful for numerical optimization and Monte Carlo simulation. It was named after the name the name of its discoverer Andre-Louis Cholesky. It is believed that cholesky decomposition is almost twice as efficient as the LU decomposition. for solving system of linear equation.

\section{Definition and Existance}
The Cholesky factorization is only defined for symmetric or Hermitian positive definite matrices.
\begin{definition}
A matrix $A\in \mathbb{R}^{m\times m}$ is symmetric positive definite (SPD) if and only if it is symmetric $(A^T=A)$ and for all nonzero vectors $x\in \mathbb{R}^m$ it is the case that $x^TAx>0$.	
\end{definition}

\begin{mybox}{Theorem}
\begin{theorem}
	Given a SPD matrix $A$ there exist a lower triangular matrix $L$ such that $A=LL^T$.
\end{theorem}
\end{mybox}
The lower triangular matrix $L$ is known as the Cholesky factor and $LL^T$ is known as the Cholesky factorization of $A$. It is unique if the diagonal elements of $L$ are restricted to be positive.

The converse holds trivially: if $A$ can be written as $LL^*$ for some invertible $L$, lower triangular or otherwise, then $A$ is Hermitian and positive definite.
\section{LDL decomposition}
Although the focus of this chapter is Cholesky factorization, it is worth to make an understanding about a closely related variant of the Cholesky factorization, the LDL decomposition.

It is represented as,

\begin{equation*}
	A = LDL^*
\end{equation*}
where $L$ is a lower triangular matrix and D is a diagonal matrix.LDL decomposition relates to Cholesky decomposition by the following way
\begin{equation*}
	\begin{aligned}
		A=LDL^*=LD^{\frac{1}{2}}D^{\frac{1}{2}*}L^*=LD^{\frac{1}{2}}(LD^{\frac{1}{2}})^*
	\end{aligned}
\end{equation*}

When LDL is efficiently implemented, it takes the same space and complexity to build and use. LDL method method is used for those cases where no Cholesky decomposition possible.

\section{Application of Choleskey Factorization}
The Cholesky decomposition is mostly used when numerical solution of linear equations $Ax = b$ is required. When $A$ is symmetric and positive definite, it is possible to solve $Ax = b$ by first computing the Cholesky decomposition $A = LL^*$, then solving $Ly = b$ for $y$ by forward substitution, and finally solving $L*x = y$ for $x$ by back substitution.

The Cholesky decomposition helps to achieve superior efficiency and numerical stability. Compared to the $LU$ decomposition, this can perform almost two times efficiently.
\subsubsection*{Linear least squares}
Systems of the form $Ax = b$ with $A$ symmetric and positive definite arise quite often in applications. For instance, the normal equations in linear least squares problems are of this form. It may also happen that matrix $A$ comes from an energy functional which must be positive from physical considerations; this happens frequently in the numerical solution of partial differential equations.
\subsubsection*{Non-linear optimization}
Non-linear multi-variate functions may be minimized over their parameters using variants of Newton's method called quasi-Newton methods. At each iteration, the search takes a step s defined by solving $Hs = -g$ for $s$, where $s$ is the step, $g$ is the gradient vector of the function's partial first derivatives with respect to the parameters, and $H$ is an approximation to the Hessian matrix of partial second derivatives formed by repeated rank 1 updates at each iteration. Two well-known update formulae are called Davidon–Fletcher–Powell (DFP) and Broyden–Fletcher–Goldfarb–Shanno (BFGS). Loss of the positive-definite condition through round-off error is avoided if rather than updating an approximation to the inverse of the Hessian, one updates the Cholesky decomposition of an approximation of the Hessian matrix itself.

\subsubsection*{Monte Carlo simulation}
The Cholesky decomposition is commonly used in the Monte Carlo method for simulating systems with multiple correlated variables: The correlation matrix is decomposed, to give the lower-triangular $L$. Applying this to a vector of uncorrelated samples, $u$, produces a sample vector $Lu$ with the covariance properties of the system being modeled.

For a simplified example that shows the economy one gets from Cholesky's decomposition, say one needs to generate two correlated normal variables $x_1$ and $x_2$. All one needs to do is to generate two uncorrelated Gaussian random variables $z_1$ and $z_2$. We set $x_1=z_1$ and $x_2 = \rho z_1 + \sqrt{1-\rho ^2 z_2}$.

\subsubsection*{Matrix inversion}
The explicit inverse of a Hermitian matrix can be computed via Cholesky decomposition, in a manner similar to solving linear systems, using $\textstyle{n^3}$ operations ($\textstyle\frac{1}{2}n^3$ multiplications). The entire inversion can even be efficiently performed in-place.

A non-Hermitian matrix $B$ can also be inverted using the following identity, where $BB^*$ will always be Hermitian:
\begin{equation*}
\mathbf{B}^{-1} = \mathbf{B}^{*} \mathbf{(B B ^ {*})}^{-1}.	
\end{equation*}

\section{The Cholesky algorithm}
Most usual algorithm for Cholesky factorization $chol(A)$ can be derived as: (\textit{Greek lowercase letters refers to scalars, lower case letter referes to vector and uppercase letters referes to matrices}). Partition
\begin{equation*}
\begin{aligned}
	A=
	\left(\begin{array}{c|c}\alpha_{11}  & \star \\ \hlinea_{21} & A_{22}\end{array}
\right)
& &
\text{ and }
L=
\left(\begin{array}{c|c}\lambda_{11}  & 0 \\ \hline\lambda_{21} & L_{22}\end{array}\right).	
\end{aligned}
\end{equation*}

By substituting these partitioned matrices into $A=LL^T$, it is found that:

\begin{equation*}
\left(\begin{array}{c|c}\alpha_{11}  & \star \\ \hlinea_{21} & A_{22}\end{array}
\right)
=
\left(\begin{array}{c|c}\lambda_{11}  & 0 \\ \hline\lambda_{21} & L_{22}\end{array}\right)
\left(\begin{array}{c|c}\lambda_{11}  & 0 \\ \hline\lambda_{21} & L_{22}\end{array}
\right)^T
=
\left(\begin{array}{c|c}\lambda_{11}^2  & \star \\ \hline\lambda_{11}l_{21} & l_{21}l_{21}^T+L_{22}L_{22}^T\end{array}
\right)
\end{equation*}

so that
\begin{equation*}
	\left(\begin{array}{c|c}\alpha_{11}=\lambda_{11}^2  & \star \\ \hlinea_{21} = \lambda_{11}l_{21} & A_{22}=l_{21}l_{21}^T+L_{22}L_{22}^T\end{array}
\right)
\end{equation*}
and hence
\begin{equation*}
	\left(\begin{array}{c|c}\lambda_{11} = \sqrt{\alpha_{11}}  & \star \\ \hlinel_{21} = a_{21}/\lambda_{11} & L_{22} = chol(A_{22}-l_{21}l_{21}^T)\end{array}
\right)
\end{equation*}
These equalities directs to the algorithm
\begin{algorithm}[h]
  \caption{Cholesky factorization}\label{euclid}
  \begin{algorithmic}[1]
    \Procedure{Cholesky}{$A$}
     \State Partition $A \gets
	\left(\begin{array}{c|c}\alpha_{11}  & \star \\ \hlinea_{21} & A_{22}\end{array}
\right)$
	\State Overwrite $\alpha_{11}:=\lambda_{11} = \sqrt{\alpha_{11}}$
	\State Overwrite $a_{21} :=l_21 = a_{21}/\lambda_11$.
	\State Overwrite $A_{22}:=A_{22}-l_{21}l_{21}^T$ (updating lower triangular part of $A_22$)
	\State Continue with $A=A_{22}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection*{Blocked Variant}
The elements of the factorization can grow arbitrarily. So to achieve high performance, the computation is cast in terms of matrix-matrix multiplication by blocked algorithms. The blocked version of the Cholesky algorithm can be derived by partitioning

\begin{equation*}
\begin{aligned}
	A=
	\left(\begin{array}{c|c}A_{11}  & \star \\ \hlineA_{21} & A_{22}\end{array}
\right)
& &
\text{ and }
L=
\left(\begin{array}{c|c}L_{11}  & 0 \\ \hlineL_{21} & L_{22}\end{array}\right).	
\end{aligned}
\end{equation*}
where $A$ and $L$ are $n_b\times n_b$ sized matrix($n_b$ block size). by substituting these into $A=LL^T$,
\begin{equation*}
\left(\begin{array}{c|c}A_{11}  & \star \\ \hlineA_{21} & A_{22}\end{array}
\right)
=
\left(\begin{array}{c|c}L_{11}  & 0 \\ \hlineL_{21} & L_{22}\end{array}\right)
\left(\begin{array}{c|c}L_{11}  & 0 \\ \hlineL_{21} & L_{22}\end{array}
\right)^T
=
\left(\begin{array}{c|c}L_{11}L_{11}^T  & \star \\ \hlineL_{21}L_{11}^T & L_{21}L_{21}^T+L_{22}L_{22}^T\end{array}
\right)
\end{equation*}
And at the end
\begin{equation*}
	\left(\begin{array}{c|c}L_{11} = cholesky({A_{11}})  & \star \\ \hlineL_{21} = A_{21}L_{11}^{-T} & L_{22} = cholesky(A_{22}-L_{21}L_{21}^T)\end{array}
\right)
\end{equation*}
So blocked variant algorithm can be described as:



\begin{algorithm}[h]
  \caption{Cholesky factorization}\label{cholesky_blk}
  \begin{algorithmic}[1]
    \Procedure{CholeskyBLK}{$A$}
     \State Partition $A \gets
	\left(\begin{array}{c|c}A_{11}  & \star \\ \hlineA_{21} & A_{22}\end{array}
\right)$
	\State Overwrite $A_{11}:=L_{11} = CholeskyBLK({A_{11}})$
	\State Overwrite $A_{21} :=L_21 = A_{21}L_{11}^T$.
	\State Overwrite $A_{22}:=A_{22}-L_{21}L_{21}^T$ (updating lower triangular part of $A_22$)
	\State Continue with $A=A_{22}$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection*{Updating the decomposition}
More often the task of updating Cholesky decomposition arises. More descriptively, at some point Cholesky decomposition $A=LL^*$ of some matrix $A$ has been computed, then a change on matrix $A$ has been made to produce matrix $\hat{A}$, and now it is required to compute the Cholesky decomposition of the new matrix: $\hat{A}=\hat{L}\hat{L}^*$. Now question arises if it is possible to reuse the Cholesky decomposition of $A$ which already has been computed to compute the Cholesky decomposition of $\hat{A}$
\subsubsection*{Rank-one update}
The situation where the updated matrix $\hat{A}$ is produced from matrix $A$ by $\hat{A}=A+xx^*$ is refers to rank-one update. 
\begin{algorithm}[h]
  \caption{Rank-one update}\label{cholesky_rank_update}
  \begin{algorithmic}[1]
    \Procedure{Choleskyrankupdate}{$L,x$}
     \State $n\gets length(x)$
     \For{$k=1:n$}
     	\State $r\gets L(power(L(k,k),2)+power(x(k),2))$
     	\State $c\gets r/L(k,k)$
     	\State $s\gets x(k)/L(k,k)$
     	\State $L(k,k)\gets r$
     	\State $L(K+1:n,k)\gets (L(k+1:n,k)+s*x(k+1:n))/c$
     	\State $x(k+1:n) \gets c*x(k+1:n) - s*L(k+1:n,k)$
     \EndFor    
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection*{Cost}
The cost of the cholesky factorization of $A\in \mathbb{R}^{m\times m}$ can be calculated from steps:
\begin{itemize}
	\item $\alpha := \sqrt{\alpha_{11}}$ negligible when k is large.
	\item $a_21 := a_{21}/\alpha_{11}$ approx. $(m-k-1)$ flops.
	\item $A_{22}:=A_{22}-triangular(a_{21}a_{21}^T)$: approx. $(m-k-1)^2$ flops
\end{itemize}
So,the total cost in form of flops is:
\begin{equation*}
	\begin{aligned}
		Cost_{Cholesky}(m) &\approx \underbrace{\sum_{k=0}^{m-1}{(m-k-1)^2} }_\textrm{Due to update of $A_{22}$} + \underbrace{\sum_{k=0}^{m-1}{(m-k-1)}}_\textrm{Due to update of$a_{21}$}\\
		&=\sum_{j=0}^{m-1}{j^2}+\sum_{j=0}^{m-1}{j}\\
		&\approx \frac{1}{3}m^3+\frac{1}{2}m^2\\
		&\approx \frac{1}{3}m^3
	\end{aligned}
\end{equation*}
It can be realised that, most computation cost is in update of $A_{22}$


\section{Sparse Cholesky Factorization Algorithm}

CVXOPT extends the built-in Python objects with two matrix objects: a {\color{red}\texttt{matrix}} object for dense 
















































